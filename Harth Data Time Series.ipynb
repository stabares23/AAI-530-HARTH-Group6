{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d7868c-5357-49cb-b5b8-b2c81b87aa52",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "The Human Activity Recognition Trondheim (HARTH) dataset comprises recordings from 22 participants, each wearing two 3-axial Axivity AX3 accelerometers for approximately 2 hours in a free-living environment. The sensors, positioned on the right thigh and lower back, capture motion data essential for human activity recognition (HAR). This dataset's rich, professionally annotated data offers a comprehensive benchmark for developing advanced machine learning models aimed at precise HAR in real-world settings.\n",
    "\n",
    "- Subject Area: Computer Science/Human Activity Recognition\n",
    "- Dataset Characteristics: Multivariate, Time Series\n",
    "- Associated Tasks: Classification\n",
    "- Feature Type: Real-valued sensor data\n",
    "- Sampling Rate: 50 Hz\n",
    "- Annotations: Activities annotated frame-by-frame using video recordings from a chest-mounted camera\n",
    "- Total Instances: 6,461,328\n",
    "- Total Features: 8, including senesor readings (back_x, back_y, back_z, thigh_x, thigh_y, thigh_z)igh_z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b261bc45-ebd3-464a-8fec-257f735a10fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sassy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from scipy import stats\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, Conv1D, MaxPooling1D, Flatten, LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, Input, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86788e-e495-43ab-a201-20155e8d2051",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f333f7-a4b2-4228-b51e-8c7d91ac62b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded S029.csv with shape: (178716, 8)\n",
      "Loaded S028.csv with shape: (165178, 8)\n",
      "Loaded S027.csv with shape: (158584, 8)\n",
      "Loaded S026.csv with shape: (195172, 8)\n",
      "Loaded S025.csv with shape: (231729, 8)\n",
      "Loaded S024.csv with shape: (170534, 8)\n",
      "Loaded S023.csv with shape: (137646, 9)\n",
      "Loaded S022.csv with shape: (337602, 8)\n",
      "Loaded S021.csv with shape: (302247, 9)\n",
      "Loaded S020.csv with shape: (371496, 8)\n",
      "Loaded S019.csv with shape: (297945, 8)\n",
      "Loaded S018.csv with shape: (322271, 8)\n",
      "Loaded S017.csv with shape: (366609, 8)\n",
      "Loaded S016.csv with shape: (355418, 8)\n",
      "Loaded S015.csv with shape: (418392, 9)\n",
      "Loaded S014.csv with shape: (366487, 8)\n",
      "Loaded S013.csv with shape: (369077, 8)\n",
      "Loaded S012.csv with shape: (382414, 8)\n",
      "Loaded S010.csv with shape: (351649, 8)\n",
      "Loaded S009.csv with shape: (154464, 8)\n",
      "Loaded S008.csv with shape: (418989, 8)\n",
      "Loaded S006.csv with shape: (408709, 8)\n",
      "Combined DataFrame shape: (6461328, 10)\n"
     ]
    }
   ],
   "source": [
    "# URL for UCI data\r\n",
    "zip_file_url = \"https://archive.ics.uci.edu/static/public/779/harth.zip\"\r\n",
    "files = [\r\n",
    "    'S029.csv', 'S028.csv', 'S027.csv', 'S026.csv', 'S025.csv',\r\n",
    "    'S024.csv', 'S023.csv', 'S022.csv', 'S021.csv', 'S020.csv',\r\n",
    "    'S019.csv', 'S018.csv', 'S017.csv', 'S016.csv', 'S015.csv',\r\n",
    "    'S014.csv', 'S013.csv', 'S012.csv', 'S010.csv', 'S009.csv',\r\n",
    "    'S008.csv', 'S006.csv',\r\n",
    "]\r\n",
    "data_types = {\r\n",
    "    'back_x': 'float64', 'back_y': 'float64', 'back_z': 'float64',\r\n",
    "    'thigh_x': 'float64', 'thigh_y': 'float64', 'thigh_z': 'float64',\r\n",
    "    'label': 'int32'\r\n",
    "}\r\n",
    "\r\n",
    "dataframes = []\r\n",
    "response = requests.get(zip_file_url)\r\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:\r\n",
    "    for file_name in files:\r\n",
    "        with zip_file.open('harth/' + file_name) as csv_file:\r\n",
    "            df = pd.read_csv(csv_file, dtype=data_types, header=0)\r\n",
    "            print(f\"Loaded {file_name} with shape: {df.shape}\")\r\n",
    "            if df.empty:\r\n",
    "                print(f\"Warning: {file_name} is empty.\")\r\n",
    "            dataframes.append(df)\r\n",
    "\r\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\r\n",
    "print(f\"Combined DataFrame shape: {combined_df.shape}\")\r\n",
    "if combined_df.empty:\r\n",
    "    raise ValueError(\"The combined DataFrame is empty. Check if the files are being loaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e54a3-5228-4394-bd8f-fc441e7f82b5",
   "metadata": {},
   "source": [
    "## Add Metadata\n",
    "Adding labels to the dataset for readability.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46da34e9-4a26-4561-9370-2641d5f3f66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'walking',\n",
       " 2: 'running',\n",
       " 3: 'shuffling',\n",
       " 4: 'stairs (ascending)',\n",
       " 5: 'stairs (descending)',\n",
       " 6: 'standing',\n",
       " 7: 'sitting',\n",
       " 8: 'lying',\n",
       " 13: 'cycling (sit)',\n",
       " 14: 'cycling (stand)',\n",
       " 130: 'cycling (sit, inactive)',\n",
       " 140: 'cycling (stand, inactive)'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_labels = {\n",
    "    1: 'walking',\n",
    "    2: 'running',\n",
    "    3: 'shuffling',\n",
    "    4: 'stairs (ascending)',\n",
    "    5: 'stairs (descending)',\n",
    "    6: 'standing',\n",
    "    7: 'sitting',\n",
    "    8: 'lying',\n",
    "    13: 'cycling (sit)',\n",
    "    14: 'cycling (stand)',\n",
    "    130: 'cycling (sit, inactive)',\n",
    "    140: 'cycling (stand, inactive)'\n",
    "}\n",
    "combined_df['label'] = combined_df['label'].map(activity_labels)\n",
    "\n",
    "# Output activity labels\n",
    "activity_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00c3cf2-d0d3-4361-91ef-45c82671b3b6",
   "metadata": {},
   "source": [
    "## Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394ccde6-0990-431a-9009-0bbc623b7c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6461328 entries, 0 to 6461327\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   timestamp  object \n",
      " 1   back_x     float64\n",
      " 2   back_y     float64\n",
      " 3   back_z     float64\n",
      " 4   thigh_x    float64\n",
      " 5   thigh_y    float64\n",
      " 6   thigh_z    float64\n",
      " 7   label      object \n",
      "dtypes: float64(6), object(2)\n",
      "memory usage: 394.4+ MB\n",
      "\n",
      "Data Head\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>back_x</th>\n",
       "      <th>back_y</th>\n",
       "      <th>back_z</th>\n",
       "      <th>thigh_x</th>\n",
       "      <th>thigh_y</th>\n",
       "      <th>thigh_z</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-12 00:00:00.000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>-0.215332</td>\n",
       "      <td>-0.997070</td>\n",
       "      <td>-0.124268</td>\n",
       "      <td>0.142334</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-12 00:00:00.020</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.084473</td>\n",
       "      <td>-0.210449</td>\n",
       "      <td>-0.964844</td>\n",
       "      <td>-0.107422</td>\n",
       "      <td>0.160645</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-12 00:00:00.040</td>\n",
       "      <td>-0.997559</td>\n",
       "      <td>-0.111328</td>\n",
       "      <td>-0.199219</td>\n",
       "      <td>-0.971191</td>\n",
       "      <td>-0.108887</td>\n",
       "      <td>0.170898</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-12 00:00:00.060</td>\n",
       "      <td>-1.006592</td>\n",
       "      <td>-0.139160</td>\n",
       "      <td>-0.209717</td>\n",
       "      <td>-0.986084</td>\n",
       "      <td>-0.112061</td>\n",
       "      <td>0.154297</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-12 00:00:00.080</td>\n",
       "      <td>-1.030029</td>\n",
       "      <td>-0.140137</td>\n",
       "      <td>-0.228760</td>\n",
       "      <td>-0.985840</td>\n",
       "      <td>-0.127441</td>\n",
       "      <td>0.155029</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 timestamp    back_x    back_y    back_z   thigh_x   thigh_y  \\\n",
       "0  2019-01-12 00:00:00.000 -1.000000 -0.071289 -0.215332 -0.997070 -0.124268   \n",
       "1  2019-01-12 00:00:00.020 -1.000000 -0.084473 -0.210449 -0.964844 -0.107422   \n",
       "2  2019-01-12 00:00:00.040 -0.997559 -0.111328 -0.199219 -0.971191 -0.108887   \n",
       "3  2019-01-12 00:00:00.060 -1.006592 -0.139160 -0.209717 -0.986084 -0.112061   \n",
       "4  2019-01-12 00:00:00.080 -1.030029 -0.140137 -0.228760 -0.985840 -0.127441   \n",
       "\n",
       "    thigh_z     label  \n",
       "0  0.142334  standing  \n",
       "1  0.160645  standing  \n",
       "2  0.170898  standing  \n",
       "3  0.154297  standing  \n",
       "4  0.155029  standing  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the original 8 features\n",
    "original_features = ['timestamp', 'back_x', 'back_y', 'back_z', 'thigh_x', 'thigh_y', 'thigh_z', 'label']\n",
    "\n",
    "# Create a temporary DataFrame with only the original 8 features\n",
    "temp_df = combined_df[original_features]\n",
    "\n",
    "# Displaying DataFrame information for the temporary DataFrame\n",
    "print('Data Info')\n",
    "temp_df.info()\n",
    "\n",
    "# Adding a visual separator for clarity\n",
    "print('\\nData Head')\n",
    "\n",
    "# Displaying the first few rows of the temporary DataFrame\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b780ad-8169-4d5a-946a-4b24c1b293c6",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Imputing missing values and performing initial data processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51ebbb9-2481-470b-975a-216e047165c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "timestamp    0\n",
      "back_x       0\n",
      "back_y       0\n",
      "back_z       0\n",
      "thigh_x      0\n",
      "thigh_y      0\n",
      "thigh_z      0\n",
      "label        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['Unnamed: 0', 'index', 'back_x_rolling_mean', 'thigh_x_rolling_mean']\n",
    "\n",
    "# Drop the columns\n",
    "combined_df = combined_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Display missing values \n",
    "print(\"Missing values per column:\")\n",
    "print(combined_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b23bd35-7101-4dc9-8711-94d66771de9b",
   "metadata": {},
   "source": [
    "## LSTM Activity Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5afbea-89b5-4627-92a4-0777dedbb60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sassy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sassy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/8\n",
      "WARNING:tensorflow:From C:\\Users\\sassy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sassy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "72690/72690 [==============================] - 317s 4ms/step - loss: 0.4163 - accuracy: 0.8667 - val_loss: 0.3779 - val_accuracy: 0.8775\n",
      "Epoch 2/8\n",
      "72690/72690 [==============================] - 312s 4ms/step - loss: 0.3704 - accuracy: 0.8787 - val_loss: 0.3655 - val_accuracy: 0.8804\n",
      "Epoch 3/8\n",
      "72690/72690 [==============================] - 312s 4ms/step - loss: 0.3616 - accuracy: 0.8809 - val_loss: 0.3587 - val_accuracy: 0.8818\n",
      "Epoch 4/8\n",
      "72690/72690 [==============================] - 304s 4ms/step - loss: 0.3564 - accuracy: 0.8822 - val_loss: 0.3558 - val_accuracy: 0.8823\n",
      "Epoch 5/8\n",
      "72690/72690 [==============================] - 315s 4ms/step - loss: 0.3529 - accuracy: 0.8831 - val_loss: 0.3542 - val_accuracy: 0.8824\n",
      "Epoch 6/8\n",
      "58546/72690 [=======================>......] - ETA: 57s - loss: 0.3505 - accuracy: 0.8836"
     ]
    }
   ],
   "source": [
    "# Encode categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "combined_df['label_encoded'] = label_encoder.fit_transform(combined_df['label'])\n",
    "\n",
    "# Define input features and target variable\n",
    "X = combined_df[['back_x', 'back_y', 'back_z', 'thigh_x', 'thigh_y', 'thigh_z']].values\n",
    "y = combined_df['label_encoded'].values\n",
    "\n",
    "# Reshape the input features for LSTM\n",
    "# LSTM input shape: [samples, time steps, features]\n",
    "# Here, we assume each sample has 1 time step\n",
    "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=64, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=8, batch_size=64, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Summarize History for Accuracy\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# Summarize History for Loss\n",
    "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abf6b4f-0d54-4d82-acbf-87ee222098e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
